{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Numpy Basics\n",
    "\n",
    "In Machine Learning and Deep Learning, we use various mathematical operations. It is important to have a set of high-performance and numerically stable implementations of those operations. This is where scientific computational libraries and packages such as NumPy play an important role. Throughout the first parts of the course, we will use NumPy to build fundamental algorithms and layers in Deep Learning.\n",
    "\n",
    "We will walk through the basics of NumPy to get some familaritiy with scientific computation libraries.\n",
    "\n",
    "### Array Construction\n",
    "\n",
    "NumPy represents data in multidimensional arrays. An array object has\n",
    "- ```dtype```: Data type such as float32, float64, int32, int64, bool, etc...\n",
    "- ```shape```: A tuple that contains the length of each dimension\n",
    "- ```size```: Total number of elements\n",
    "- ```ndim```: Number of dimensions of the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Callable\n",
    "import numpy as np\n",
    "\n",
    "scalar = np.float32(1/3)\n",
    "vector = np.array([0, 1, 2.])\n",
    "matrix = np.array([[0, 1, 2], [0, 2, 3]], dtype=np.float64)\n",
    "tensor = np.array([[[0, 1], [0, 2]], [[0, 3.0], [0, 4]]], dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although you can create an array using Python lists, there are other convenient NumPy functions to fill and create arrays of different shapes and values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones_vector = np.ones(shape=5, dtype=np.int32, order=\"C\")\n",
    "zeros_matrix = np.zeros(shape=(6, 6))\n",
    "empty_tensor = np.empty(shape=(2, 2, 3), dtype=np.float32)\n",
    "zeros_tensor = np.zeros_like(empty_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can create ranged arrays using [```np.arange```](https://numpy.org/doc/stable/reference/generated/numpy.arange.html) and [```np.linspace```](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html). Please read the documentation of these functions for further details.\n",
    "\n",
    "> Implement ```alternative_linspace``` using ```np.arange``` without ```np.linspace``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternative_linspace(start: float, stop: float, num: int) -> np.ndarray:\n",
    "    \"\"\" Linspace\n",
    "\n",
    "    Args:\n",
    "        start (float): starting value\n",
    "        stop (float): end value\n",
    "        num (int): number of elements\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: ranged array between <start> and <stop> values of size <num>\n",
    "    \"\"\"\n",
    "\n",
    "#assert(np.allclose(alternative_linspace(-1, 1, 50), np.linspace(-1, 1, 50)))\n",
    "#assert(np.allclose(alternative_linspace(-5, 5, 1), np.linspace(-5, 5, 1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge and Split\n",
    "\n",
    "We can merge arrays using [np.concatenate](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) and [np.stack](https://numpy.org/doc/stable/reference/generated/numpy.stack.html). Similarly, we divide arrays into multiple sub-arrays with [np.split](https://numpy.org/doc/stable/reference/generated/numpy.split.html) and [np.array_split](https://numpy.org/doc/stable/reference/generated/numpy.array_split.html).\n",
    "\n",
    "> Implement ```combine_feautures``` and ```data_split``` functions using the NumPy functions mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_feautures(first_features: np.ndarray, second_features: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Combine two features into a single array.\n",
    "\n",
    "    Args:\n",
    "        first_features (np.ndarray): shape: (B, D)\n",
    "        second_features (np.ndarray): shape: (B, F)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: shape: (B, D+F)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "#assert(np.allclose(combine_feautures(np.array([[1, 1], [2, 2]]), np.array([[3, 3], [4, 4]])), np.array([[1, 1, 3, 3], [2, 2, 4, 4]])))\n",
    "    \n",
    "def data_split(data: np.ndarray, train_split_ratio: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\" Split <data> into train and test data based on the value of <train_split_ratio>.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): data array of shape : (B, D)\n",
    "        train_split_ratio (float): Ratio of train samples to whole data\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray]: Tuple of train and test samples. Shapes: (K, D), (L, D) where K/(K+L) = <train_split_ratio>\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "#part_1, part_2 = data_split(np.concatenate([np.zeros((6, 4)), np.ones((6, 4))], axis=0), 0.5)\n",
    "#assert(np.allclose(part_1, np.zeros((6, 4))))\n",
    "#assert(np.allclose(part_2, np.ones((6, 4))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Arrays\n",
    "\n",
    "NumPy provides random array construction for a variety of distributions.\n",
    "- ```np.random.rand``` Uniform random between 0 and 1\n",
    "- ```np.random.randn``` Standard Normal\n",
    "- ```np.random.randint``` Discrete Uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_labels = np.random.randint(0, 2, size=(12,))\n",
    "random_weights = np.random.randn(16, 16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape and Order\n",
    "\n",
    "NumPy represents an array as a contiguous block in memory. The shape or dimensions of an array does not change how it's represented in the memory. We can modify the shape of an array or add and remove dimensions to/from it without reallocation or modification in the array memory (O(1) complexity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5]]),\n",
       " array([[0, 1, 2, 3, 4, 5]]),\n",
       " array([[0, 1, 2],\n",
       "        [3, 4, 5]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.arange(6)  # Shape: (6,)\n",
    "matrix = array.reshape(2, 3)  # Shape: (2, 3)\n",
    "column_vector = np.expand_dims(array, axis=1)  # Shape: (6, 1)\n",
    "row_vector = np.expand_dims(array, axis=0)  # Shape: (1, 6)\n",
    "\n",
    "#assert(np.allclose(array, np.squeeze(row_vector, axis=0)))\n",
    "column_vector, row_vector, matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NumPy, arrays are ordered by row as default (similar to \"C\" representation and contrary to \"Fortran\" representation). If we want to change this ordering, for example, with ```transpose```, NumPy changes the order instantly without modifing the array memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 1, 2],\n",
       "        [3, 4, 5]]),\n",
       " array([[0, 2, 4],\n",
       "        [1, 3, 5]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.arange(6)\n",
    "row_first_matrix = array.reshape(2, 3)  # By default C order\n",
    "column_first_matrix = array.reshape(2, 3, order=\"F\")  # F for Fortran\n",
    "\n",
    "row_first_matrix, column_first_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the order as shown above. For matrices, this corresponds to transpose operation. However, for multidimensional arrays (tensors), we need to specify the new order in the ```transpose``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       " \n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]]),\n",
       " array([[[ 0, 12],\n",
       "         [ 4, 16],\n",
       "         [ 8, 20]],\n",
       " \n",
       "        [[ 1, 13],\n",
       "         [ 5, 17],\n",
       "         [ 9, 21]],\n",
       " \n",
       "        [[ 2, 14],\n",
       "         [ 6, 18],\n",
       "         [10, 22]],\n",
       " \n",
       "        [[ 3, 15],\n",
       "         [ 7, 19],\n",
       "         [11, 23]]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.arange(24)\n",
    "tensor = array.reshape(2, 3, 4)  # Shape: (2, 3, 4)\n",
    "transposed_tensor = np.transpose(tensor, (2, 1, 0))  # Shape: (4, 3, 2)\n",
    "\n",
    "tensor, transposed_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "NumPy supports Python indexing and slicing. That is, we can use slicing like ```array[::2]``` and ```array[:-2]```. Since NumPy arrays can be multidimensional, we can index or slice each dimension separately (```array[::2, :-3, 0]```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0,  2],\n",
       "        [ 8, 10]]),\n",
       " array([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7]],\n",
       " \n",
       "        [[ 8,  9, 10, 11],\n",
       "         [12, 13, 14, 15]],\n",
       " \n",
       "        [[16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.arange(4 * 3 * 2).reshape(3, 2, 4)\n",
    "array[:-1, 0, ::2], array "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Indexing\n",
    "\n",
    "Simple slices may not be enough for some tasks. If we need to take values of specific indices we can use arrays for indexing (```array[np.array([2, 3, 6])]```). We can also obtain values that satisfy a condition. For example, all positive values ```array[array > 0]```. Please read the [documentation](https://docs.scipy.org/doc/numpy-1.10.0/reference/arrays.indexing.html) for further details.\n",
    "\n",
    "In multilabel classification tasks, we often need a function that can convert labels vector into a sparse matrix, which we call one-hot representation, where each row is a vector that contains one \"1\" at the index specified by the corresponding label. For example:\n",
    "\n",
    "- number of classes: 5\n",
    "\n",
    "- label:\n",
    "\\begin{bmatrix}\n",
    "0 & 4 & 2 & 1\n",
    "\\end{bmatrix}\n",
    "\n",
    "- onehot_matrix: \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 1\\\\\n",
    "0 & 0 & 1 & 0 & 0\\\\\n",
    "0 & 1 & 0 & 0 & 0\\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "\n",
    "> Implement the ```one_hot``` function using advanced indexing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels: np.ndarray, n_labels: int) -> np.ndarray:\n",
    "    \"\"\" Convert labels to one-hot matrix\n",
    "\n",
    "    Args:\n",
    "        labels (np.ndarray): 1D integer vector\n",
    "        n_labels (int): Number of classes/(unique labels)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: One-hot matrix of the given <labels> vector\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    one_hot_matrix = np.zeros((len(labels), n_labels))\n",
    "    \n",
    "    one_hot_matrix[np.arange(len(labels)), labels] = 1\n",
    "    \n",
    "    return one_hot_matrix\n",
    "assert(np.allclose(one_hot(np.array([1, 0, 2, 2]), 3),\n",
    "                   np.array([[0, 1, 0], [1, 0, 0], [0, 0, 1], [0, 0, 1]])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting\n",
    "\n",
    "Probably, broadcasting is one of the most taken-for-granted functionalities of NumPy. It automatically matches the shapes of two arrays when possible before element-wise operations. For example, let the two arrays have the shape (K, L) and (K, 1). Whenever an element-wise operation is called upon these to arrays, instead of raising shape mismatch exception, NumPy applies broadcasting operation and adds additional axes or repeats the array over some axes if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_array = np.ones((5, 4))\n",
    "second_array = np.ones((5, 1))\n",
    "\n",
    "auto_broadcasted_array = first_array + second_array  # Shape: (5, 4)\n",
    "manual_broadcasted_array = first_array + np.repeat(second_array, 4, axis=1)  # Shape: (5, 4)\n",
    "\n",
    "assert(np.allclose(auto_broadcasted_array, manual_broadcasted_array))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, NumPy automatically repeats the ```second_array``` at the second dimension before applying addition.\n",
    "\n",
    "#### One-hot with broadcasting example\n",
    "\n",
    "We can use broadcasting to create a one-hot matrix as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.]], dtype=float32),\n",
       " array([2, 1, 1, 0, 2, 1]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.random.randint(0, 3, size=6)  # Shape: (6)\n",
    "range_row_vector = np.arange(3).reshape(1, 3)  # Shape: (1, 3)\n",
    "label_column_vector = labels.reshape(-1, 1)  # Shape: (6, 1)\n",
    "onehot_matrix = (range_row_vector == label_column_vector).astype(np.float32)  # Shape: (6, 3)\n",
    "\n",
    "onehot_matrix.astype(np.float32), labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Pitfalls of auto-broadcasting\n",
    "\n",
    "Automatic broadcasting may lead to unexpected results. What would happen if we sum an array of shape (K, 1) with an array of shape (K)? The resulting array has the shape (K, K).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 2., 2., 2., 2.],\n",
       "       [2., 2., 2., 2., 2.],\n",
       "       [2., 2., 2., 2., 2.],\n",
       "       [2., 2., 2., 2., 2.],\n",
       "       [2., 2., 2., 2., 2.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_array = np.ones((5, 1))\n",
    "second_array = np.ones(5)\n",
    "\n",
    "first_array + second_array  # Shape: (5, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regression tasks, the prediction array of our models generally has the shape of (B, 1), where \"B\" denotes the batch axis. But the ground truth values may have the shape (B). That leads to the former issue. \n",
    "\n",
    "> Implement ```sum_square_loss``` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22960\\4054703518.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum_square_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m5.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def sum_square_loss(prediction: np.ndarray, ground_truth: np.ndarray) -> np.float32:\n",
    "    \"\"\" Calculate the square distance over the elements of the given arrays and return the sum of errors.\n",
    "\n",
    "    Args:\n",
    "        prediction (np.ndarray): Prediction array of shape (B, 1)\n",
    "        ground_truth (np.ndarray): Array of true values which has the shape (B)\n",
    "\n",
    "    Returns:\n",
    "        np.float32: Scalar loss summed alonged the batch axis \"B\"\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    return sum_square_loss\n",
    "    \n",
    "\n",
    "assert(sum_square_loss(np.ones((5, 1)), np.zeros(5)) == 5.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Functions like ```np.sum```, ```np.max```, and ```np.mean``` reduce the dimension of an array at the dimension that they operate. If you want to keep that dimension as a dummy dimension of \"1\" you can use ```keepdims``` parameter of those functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Implement ```normalization``` function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Normalize the data array by subtracting it from its mean and dividing it by its standard deviation \n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Data array of shape (B, D)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Normalized array of shape (B, D)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    \n",
    "    return normal_data\n",
    "\n",
    "assert(np.allclose(normalization(np.arange(6, dtype=np.float32).reshape(3, 2)),\n",
    "                   np.array([[-1.2247448, -1.2247448],\n",
    "                             [0.,  0.],\n",
    "                             [1.2247448,  1.2247448]], dtype=np.float32)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use broadcasting, element-wise math (summation, multiplication, ...) and reduce operations (sum, mean, max, std, ...) to implement \"slow\" matrix multiplication and many other layers in Deep Learning.\n",
    "\n",
    "> Implement ```slow_matrix_multiplication``` using the aforementioned operations. **Do not** use matrix multipliation functions of NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def slow_matrix_multiplication(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Matrix multiplication with element-wise multiplication, sum, and reshape/expand_dims\n",
    "    \n",
    "    Args:\n",
    "        first_matrix (np.ndarray): 2D array of shape (K, L)\n",
    "        second_matrix (np.ndarray): 2D array of shape (L, T)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: resultant matrix of shape (K, T) \n",
    "    \"\"\"\n",
    "\n",
    "    return matrix_multiplication\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "first_matrix = np.random.randn(6, 4)\n",
    "second_matrix = np.random.randn(4, 8)\n",
    "assert(np.allclose(slow_matrix_multiplication(first_matrix, second_matrix), first_matrix @ second_matrix))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einsum\n",
    "Alternatively, there is a very convenient function that NumPy provides. For most of the mathematical operations ```np.einsum``` is all you need. Although it is not mandatory, we highly suggest that you take a look at the [documentation](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html) of the einsum function and try to use it in upcoming homework.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6ee2ecf61b74c3b0e997a17b9194eac603566e9117375fa5485c0b29d12eba50"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

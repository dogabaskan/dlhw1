{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Logistic Regression\n",
    "\n",
    "In this part, we will implement a logistic regression classifier and train it with [fashion MNIST dataset](https://github.com/zalandoresearch/fashion-mnist) using stochastic gradient descent (SGD).\n",
    "\n",
    "First, download the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from typing import Tuple\n",
    "import requests\n",
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from src.logistic_regression import DataLoader, LogisticRegresssionClassifier\n",
    "from src.logger import Logger\n",
    "\n",
    "\n",
    "train_labels_url = \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\"\n",
    "train_images_url = \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\"\n",
    "test_labels_url = \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\"\n",
    "test_images_url = \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\"\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "\n",
    "def download_and_load(url: str, name: str, kind: str, offset: int) -> np.ndarray:\n",
    "    file_path = f\"data/{kind}_{name}\"\n",
    "    if not os.path.exists(file_path):\n",
    "        response = requests.get(url)\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "    with gzip.open(file_path, \"rb\") as lbpath:\n",
    "        return np.frombuffer(lbpath.read(), dtype=np.uint8, offset=offset)\n",
    "\n",
    "\n",
    "train_labels = download_and_load(train_labels_url, \"train\", \"labels\", 8)\n",
    "train_data = download_and_load(train_images_url, \"train\", \"images\",\n",
    "                               16).reshape(len(train_labels), 784)\n",
    "test_labels = download_and_load(test_labels_url, \"test\", \"labels\", 8)\n",
    "test_data = download_and_load(test_images_url, \"test\", \"images\", 16).reshape(len(test_labels), 784)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "In this part, we will complete the ```LogisticRegresssionClassifier``` and ```DataLoader``` classes under the \"src/logistic_regression.py\" file. The classifier has two main methods, ```predict``` and ```fit```, similar to scikit API. Please take a look at ```fit``` method which is already completed. This method contains the main loop of the learning process.\n",
    "\n",
    "We start the implementation with the data loader. Since we are using SGD, we need to partition the data into batches. Also, at the start of every epoch (a full pass of the data), we shuffle the data by randomly permuting it.\n",
    "\n",
    "> Complete ```DataLoader``` by implementing ```shuffle``` and ```__next__``` methods.\n",
    "\n",
    "**Note:** You may want to read this [article](http://www.trytoprogram.com/python-programming/python-iterators/) about ```__iter__``` method  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, train_labels, batch_size=32)\n",
    "iterator = iter(train_loader)\n",
    "batch_data, batch_label = next(iterator)\n",
    "assert(batch_data.shape == (32, 784))\n",
    "assert(batch_label.shape == (32,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we complete ```LogisticRegresssionClassifier``` starting with ```_initialize``` method. This method initiates the weights and biases of the classifier.\n",
    "\n",
    "> Complete ```_initialize``` methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegresssionClassifier(784, 10)\n",
    "assert(classifier.weights.shape == (784, 10))\n",
    "assert(classifier.bias.shape == (10,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "In the classification task, we use conditional class probabilities $P(Y\\vert X, \\theta)$ where $\\theta$ denotes the parameters. We use maximum likelihood estimation (MLE) to find the best fit for the training data. However, contrary to Linear Regression models, in Logistic Regression, it is not straightforward to calculate the best fitting parameter $\\theta$ with non-iterative methods due to non-linearity at the end of the function. Therefore, we use Stochastic Gradient Descent to iteratively maximize the likelihood of the data. Please read [chapters 5.5 and 5.9 of Deep Learning Book](https://www.deeplearningbook.org/contents/ml.html) for details.\n",
    "\n",
    "Since SGD minimizes the objective and log is a monotonic function, we can use negative log-likelihood (NLL) loss to maximize the likelihood.\n",
    "\n",
    "> Complete ```nll_loss``` methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = np.ones((1, 5)) /5 \n",
    "label = np.array([3])\n",
    "\n",
    "assert(np.allclose(classifier.nll_loss(pred_probs, label), -np.log(pred_probs[0, 3])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "Next, we implement ```predict``` method to obtain conditional class probabilities $P(Y \\vert X, \\theta)$. In order to do that, we need to implement Softmax function that models a categorical distribution over the classes.\n",
    "\n",
    "Before moving to implementation, calculate the jacobian of the softmax function $f(\\bm{x}): \\mathcal{R}^{n} \\rightarrow \\mathcal{R}^{n}$. You can upload an image of your solution or write it in latex.\n",
    "\n",
    "> Calculate the jacobian of the softmax function\n",
    "\n",
    "<!-- ![solution](Jacobian_Softmax_Function.png) -->\n",
    "<img src=\"Jacobian_Softmax_Function.png\" alt=\"drawing\" high=\"200\" width=\"1000\"/>\n",
    "\n",
    "> Complete ```softmax``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(np.allclose(classifier.softmax(np.ones((1, 5))), np.ones((1, 5)) / 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Complete ```predict``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert(classifier.predict(next(iterator)[0]).shape == (32, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 784)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iterator)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following that, we can implement accuracy metric functions.\n",
    "\n",
    "> Complete ```accuracy``` and ```confusion_matrix``` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(classifier.accuracy(np.array([0, 1, 0, 0]), np.array([0, 1, 0, 1])) == 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert((classifier.confusion_matrix(np.array([0, 1, 0, 0]), np.array([0, 1, 0, 1]),2) == np.array([[2, 1], [0, 1]])).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not: confusion_matrix fonksiyonuna model.n_classes inputunu ekledim. labels ve predictions inputlarının confusion_matrix'in boyutunu belirlerken yeterli olamayacağı durumlar olduğunu düşündüm. Örneğin labels veya predictions larıma bakararak unique değerlerin sayısına göre confusion_matrix boyutunu belirleyebilirdim fakat labels veya predictions'da olmayan durumlar olabilirdi. Bu sebeple generic bir yapıya dönüştürmek istedim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning\n",
    "\n",
    "Now that we completed the prediction and data loader parts, we can start completing the requisite methods for ```fit```. In order to update parameters, we need to calculate gradients of the loss function with respect to weights and biases.\n",
    "\n",
    "> Implement ```nll_gradients``` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.array(1, dtype=np.int32)\n",
    "\n",
    "inccorect_pred = np.zeros((1, 10), dtype=np.float32)\n",
    "inccorect_pred[0, 0] = 1.0\n",
    "\n",
    "correct_pred = np.zeros((1, 10), dtype=np.float32)\n",
    "correct_pred[0, 1] = 1.0\n",
    "\n",
    "\n",
    "\n",
    "inputs = np.ones((1, 784), dtype=np.float32)\n",
    "\n",
    "weight_grad, bias_grad = classifier.nll_gradients(probs=correct_pred, inputs=inputs, labels=label)\n",
    "assert((weight_grad == 0).all())\n",
    "assert((bias_grad == 0).all())\n",
    "\n",
    "weight_grad, bias_grad = classifier.nll_gradients(probs=inccorect_pred, inputs=inputs, labels=label)\n",
    "\n",
    "assert((weight_grad[:, 0] == 1.0).all() and (weight_grad[:, 1] == -1.0).all() and (weight_grad[:, 2:] == 0.0).all())\n",
    "assert((bias_grad[0] == 1.0) and (bias_grad[1] == -1.0) and (bias_grad[2:] == 0.0).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to implement the ```update``` method that updates the parameters with the given gradient and l2 regularization.\n",
    "\n",
    "> Implement ```update``` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_weights = classifier.weights.copy()\n",
    "prev_bias = classifier.bias.copy()\n",
    "classifier.update((np.ones_like(prev_weights), np.ones_like(prev_bias)), 1.0, 0.0)\n",
    "\n",
    "assert(np.allclose(classifier.weights, prev_weights -1.0))\n",
    "assert(np.allclose(classifier.bias, prev_bias -1.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we finalized our implementation, we can train our classifier on the fashion MNIST dataset. Split the train data into train and eval sets and create ```train_loader``` and ```eval_loader```.\n",
    "\n",
    "> Implement ```split_dataset``` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8155122a0044f9ca71e244a18509f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FigureWidget({\n",
       "    'data': [{'mode': 'markers+lines',\n",
       "              'name': 'accuracy',\n",
       "       …"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_dataset(data: np.ndarray, label: np.ndarray, batch_size: int, train_ratio: float = 0.9\n",
    "                  ) -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\" Split the data into train and eval sets\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Data array of shape (B, D)\n",
    "        label (np.ndarray): Label array os shape (B)\n",
    "        batch_size (int): Batch size of the dataloaders\n",
    "        train_ratio (float): Ratio of the train sample size to overall sample size \n",
    "\n",
    "    Returns:\n",
    "        Tuple[DataLoader, DataLoader]: Train and Eval Dataloaders\n",
    "    \"\"\"\n",
    "    z = int(data.shape[0] * train_ratio)\n",
    "    \n",
    "    train_data, eval_data = data[:z], data[z:]\n",
    "    train_labels, eval_labels = label[:z], label[z:]\n",
    "    \n",
    "    train_loader = DataLoader(train_data, train_labels, batch_size=32)\n",
    "    eval_loader = DataLoader(eval_data, eval_labels, batch_size=32)\n",
    "    \n",
    "    return train_loader, eval_loader\n",
    "\n",
    "@dataclass\n",
    "class Hyperparameters():\n",
    "    train_eval_ratio: float = 0.8\n",
    "    batch_size: int = 32\n",
    "    learning_rate: float = 1e-3\n",
    "    l2_coeff: float = 1e-1\n",
    "    epoch: int = 20\n",
    "\n",
    "\n",
    "hyperparams = Hyperparameters()\n",
    "\n",
    "train_loader, eval_loader = split_dataset(train_data, train_labels, hyperparams.batch_size)\n",
    "logger = Logger(smooth_window_len=100, verbose=False, live_figure_update=True)\n",
    "logger.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run the cell above, if you do not encounter any exception, you must see two empty plots side by side. When model ```fit``` function calls loggers ```log_iteration``` and ```log_epoch``` methods, these plots will be updated with the new values (given that logger is initailized with ```live_figure_update=True```). You can also use ```verbose``` option that allows logger to prints out the logs.\n",
    "\n",
    "#### Start Training\n",
    "\n",
    "You can start model training by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegresssionClassifier(784, 10)\n",
    "model.fit(train_loader,\n",
    "          eval_loader,\n",
    "          hyperparams.learning_rate,\n",
    "          hyperparams.l2_coeff,\n",
    "          hyperparams.epoch,\n",
    "          logger)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, classifier fits the data with more than 80% evaluation accuracy. If so, you can run the model with the test data using the Test Loader and observe the confusion matrix in the test data.\n",
    "\n",
    "> Implement ```test_classifier``` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(data_loader: DataLoader, model: LogisticRegresssionClassifier) -> np.ndarray:\n",
    "    \"\"\" Run the model with test data loader and return confusion matrix\n",
    "\n",
    "    Args:\n",
    "        data_loader (DataLoader): Data loader of the test data\n",
    "        model (LogisticRegresssionClassifier): Trained classifier\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Confusion matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    cm = np.zeros((model.n_classes,model.n_classes)) \n",
    "    for iter_index, (data, labels) in enumerate(data_loader):\n",
    "        probs = model.predict(data)\n",
    "        predictions = probs.argmax(axis=-1)\n",
    "        cm += model.confusion_matrix(labels,predictions,model.n_classes)\n",
    "    return cm\n",
    "\n",
    "\n",
    "test_loader = DataLoader(test_data, test_labels, hyperparams.batch_size)\n",
    "confusion_matrix = test_classifier(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lets run the cell below to plot ```confusion_matrix```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverongaps": false,
         "type": "heatmap",
         "z": [
          [
           753,
           2,
           13,
           63,
           4,
           11,
           140,
           0,
           13,
           1
          ],
          [
           4,
           944,
           5,
           32,
           6,
           0,
           7,
           1,
           1,
           0
          ],
          [
           13,
           3,
           683,
           18,
           131,
           6,
           135,
           0,
           11,
           0
          ],
          [
           18,
           12,
           14,
           855,
           39,
           1,
           58,
           1,
           2,
           0
          ],
          [
           1,
           2,
           101,
           31,
           736,
           5,
           117,
           0,
           7,
           0
          ],
          [
           0,
           0,
           0,
           3,
           0,
           888,
           0,
           58,
           7,
           44
          ],
          [
           103,
           2,
           106,
           46,
           94,
           12,
           615,
           0,
           22,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           48,
           0,
           890,
           2,
           60
          ],
          [
           3,
           1,
           12,
           16,
           0,
           17,
           31,
           14,
           904,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           23,
           1,
           40,
           2,
           934
          ]
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "scatter": [
           {
            "type": "scatter"
           }
          ]
         }
        },
        "title": {
         "text": "Confusion Matrix"
        },
        "width": 500,
        "xaxis": {
         "title": {
          "text": "True Labels"
         }
        },
        "yaxis": {
         "autorange": "reversed",
         "title": {
          "text": "Predicted Labels"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger.render_confusion_matrix(confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6ee2ecf61b74c3b0e997a17b9194eac603566e9117375fa5485c0b29d12eba50"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
